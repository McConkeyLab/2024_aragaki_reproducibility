= Science has a reproducibility problem
Science is incremental in nature, and relies on tuning, revising, and refuting previous models. Because of this, the reliability of the information being built upon is central to this endeavor. Needing to assume published information to be correct without replication is sometimes unavoidable --- particularly in the case of large studies or studies with very rare patients, where it would be impractical or impossible to replicate. Even in cases where replication is possible, it is impossible to tell if failure to replicate previously published results is the true reality (that is, the published results were in error) or technical differences preclude reproducibility.
In recent years, a variety of replication studies have been performed, with the vast majority of studies failing to be replicated. Investigators at Bayer and Amgen were unable to reproduce results in 65% and 89% of cases, respectively @Prinz_2011 @Begley_2012, with other studies presenting similar results (one review indicating a range of 75--90% of preclinical studies failed to replicate@Begley_2015). If we refuse to accept this as an unavoidable aspect of science, we must accept that this is a source of immense waste of time, effort, material, and funding.

= Science has an _efficiency_ problem
Replication and research efficiency are linked by their nature: Poor research produces poorly reproducible or erroneous results, which in turn leads investigators to waste time and effort either attempting to reproduce or build off of previously determined results. An estimated 85% of research effort is 'wasted' through various points in the scientific process @Chalmers_2009, and a series of articles in the _Lancet_ reports on the various stages in which research efficiency can be improved, including study planning, publishing, regulation, and reporting @Chalmers_2014 @Ioannidis_2014 @Salman_2014 @Chan_2014 @Glasziou_2014. From this series, it is clear that the sources of waste are from all aspects of the scientific process. While a single 'silver bullet' solution is not possible, it also means that solutions can and must come from a variety of sources, including from within our own labs.

In the series presented by the _Lancet_, Ioannidis et al.@Ioannidis_2014 mention inefficiencies brought by reporting and analysis. In particular, they mention both the shocking frequency of incorrect p--value reporting (at least one incongruent p--value in 38% of papers in _Nature_, vol 409--412@Garcia_Berthou_2004), as well as the poor availability of protocols. While basic science protocols tend to be more readily available in materials and methods sections of papers, these abbreviated formats seem to be insufficient to easily reproduce the experiment --- otherwise, we may expect journals like _Nature Protocols_ to be much shorter. Other fields have noted the insufficiency of this format as well. In his address, former president of the Royal Statistical Society Peter James Green asserted that the current format of articles did not allow for sufficient detail to reproduce the results shown in the paper@Green_2003. Since his address in 2003, methods have only become more complex, yet method reporting has largely remained the same.

Access to information also defines an area for improvement. There are an assumed vast amount of data that remain unpublished due to negative results@Rosenthal_1979, despite being informative for other investigators in the field or --- in the case of clinical trials --- patients. This is compounded by the barrier--to--entry to many journals of the expense of both publishing and access to papers. While pre--print servers like bioRxiv exist, research implies that authors may be self--filtering their manuscript submissions with null results, rather than journals explicitly selecting manuscripts with significant results more regularly than non-significant results@Chan_2014. This may imply that the issue must be addressed at a psychological and social level first.

Avenues to overcome these challenges have been offered for clinical research, but little infrastructure exists for basic research. This may be in part due to the heterogeneity of basic research, where 'endpoints' collected are varied and growing as new experiments and assays are performed and phenomena are discovered. This is an 'on-target' effect of basic research, and trying to mold it to fit the structures created for clinical research would be a detriment to the process. To enhance reproducibility and efficiency, our solutions must be able to mold to our research, not vice versa.

= Someone needs to write code. It doesn't have to be you.
Code is a powerful tool for reproducibility. Code can define how data becomes a plot, and can perform analyses with consistency in a way that hand calculation can fail. When shared, it provides an exact window into what was done, and provides an easy way for others to repeat the same analysis (or perhaps a similar analysis with different data). However, writing code is not easy. While myriad high--quality sources for learning to code exist, learning to code takes time and effort, and learning best practices takes longer still. Depending on the situation, it may not be efficient for all members of the lab to become proficient in learning how to code. Rather, it may be more beneficial for there to be a specialist that is capable of producing easy--to--use interfaces that help facilitate best practices at no hassle to the user, via abstraction.

A car salesman is the daily subject to the effects of quantum mechanics, as the entire universe is dictated by it. However, to be a successful car salesman, he needs to know no quantum physics. This is an (extreme) example of (unintentional) abstraction: being able to perform high level tasks without having to understand the particulars of how they happen. It is currently not possible --- and likely never will be --- to understand everything down to the fundamental level. Abstraction is a necessity, and we should treat it as a small gift rather than a nuisance.

Through abstractions, bench scientists can successfully and easily use applications to help automate their tasks, create reports, and generate reproducible scripts, all without needing to understand how the underlying code works. For this to be fruitful, there must be some overlap in domain knowledge between the bench scientists and developer. A developer who does not understand what a bench scientists wants and what their workflows will produce unnecessary or irritating software that will --- at best --- not be used.

In the realm of software development, DevOps (a portmanteau of "software development" and "IT operations") is a loosely defined term referring --- broadly --- to the practice of decreasing development time of robust software via the creation of tooling particular for the team or company in which they work. I believe a similar role should exist for laboratories.

At the McConkey Lab, I had the opportunity to work simultaneously in a wet--lab and dry--lab setting, in a lab that contained largely short term members (1 year) that were primarily focused on wet--lab duties with little to no coding experience. This created a unique opportunity for reproducibility to be enhanced, but with a team that did not have the time to learn how to code. During this time, I developed software infrastructure and learned what things tended to work, and what things were simply lofty ideas. The following sections are descriptions and case studies of some of the software and infrastructure I have developed in my time at the McConkey lab.
